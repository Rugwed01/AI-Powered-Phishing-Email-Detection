{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32694435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check CUDA availability (important for PyTorch & Transformers)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba3dff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load and shuffle your dataset\n",
    "df = pd.read_csv(\"C:/Users/rugwe/OneDrive/Desktop/AI_LAB/Datasets/Final.csv\")  # Replace with your path\n",
    "df = shuffle(df, random_state=42)\n",
    "\n",
    "X = df['text']  # Replace with actual text column name\n",
    "y = df['labels']\n",
    "\n",
    "# Train-Test Split (80/20)\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8045bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_raw)\n",
    "X_test_tfidf = tfidf.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d46b6896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:25<00:00,  1.96it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract RoBERTa Embeddings (CUDA, Batched)\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\").to(\"cuda\")\n",
    "roberta_model.eval()\n",
    "\n",
    "def get_roberta_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = roberta_model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_embeddings.cpu())\n",
    "    return torch.cat(embeddings).numpy()\n",
    "\n",
    "X_train_bert = get_roberta_embeddings(X_train_raw.tolist())\n",
    "X_test_bert = get_roberta_embeddings(X_test_raw.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3bc84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine All Feature Sets (Final Hybrid)\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# You can choose which features to include\n",
    "# Only sparse + dense (use hstack only for sparse)\n",
    "X_train_combined = hstack([X_train_tfidf])  # Add others like X_train_w2v, X_train_bert\n",
    "X_test_combined = hstack([X_test_tfidf])    # Match same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf9d4990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Random Forest Accuracy: 0.9975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       197\n",
      "           1       1.00      1.00      1.00       203\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n",
      "âš¡ XGBoost Accuracy: 0.9825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       197\n",
      "           1       0.98      0.99      0.98       203\n",
      "\n",
      "    accuracy                           0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "ðŸ§© SVM Accuracy: 0.9925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       197\n",
      "           1       0.99      1.00      0.99       203\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       0.99      0.99      0.99       400\n",
      "weighted avg       0.99      0.99      0.99       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train_combined, y_train)\n",
    "rf_preds = rf.predict(X_test_combined)\n",
    "print(\"ðŸŒ² Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "print(classification_report(y_test, rf_preds))\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier(eval_metric='logloss')\n",
    "xgb.fit(X_train_combined, y_train)\n",
    "xgb_preds = xgb.predict(X_test_combined)\n",
    "print(\"âš¡ XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n",
    "print(classification_report(y_test, xgb_preds))\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train_combined, y_train)\n",
    "svm_preds = svm.predict(X_test_combined)\n",
    "print(\"ðŸ§© SVM Accuracy:\", accuracy_score(y_test, svm_preds))\n",
    "print(classification_report(y_test, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04386438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/svm_model.pkl']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Everything\n",
    "import joblib\n",
    "joblib.dump(tfidf, \"models/tfidf_vectorizer.pkl\")\n",
    "joblib.dump(w2v_model, \"models/w2v_model.pkl\")\n",
    "joblib.dump(rf, \"models/rf_model.pkl\")\n",
    "joblib.dump(xgb, \"models/xgb_model.pkl\")\n",
    "joblib.dump(svm, \"models/svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a2a1fee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Random Forest Cross-Validation Accuracy: 0.9881 Â± 0.0050\n",
      "âš¡ XGBoost Cross-Validation Accuracy: 0.9838 Â± 0.0085\n",
      "ðŸ§© SVM Cross-Validation Accuracy: 0.9875 Â± 0.0052\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Cross-validation for Random Forest\n",
    "rf_cv_scores = cross_val_score(rf, X_train_combined, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"ðŸŒ² Random Forest Cross-Validation Accuracy: {np.mean(rf_cv_scores):.4f} Â± {np.std(rf_cv_scores):.4f}\")\n",
    "\n",
    "# Cross-validation for XGBoost\n",
    "xgb_cv_scores = cross_val_score(xgb, X_train_combined, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"âš¡ XGBoost Cross-Validation Accuracy: {np.mean(xgb_cv_scores):.4f} Â± {np.std(xgb_cv_scores):.4f}\")\n",
    "\n",
    "# Cross-validation for SVM\n",
    "svm_cv_scores = cross_val_score(svm, X_train_combined, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"ðŸ§© SVM Cross-Validation Accuracy: {np.mean(svm_cv_scores):.4f} Â± {np.std(svm_cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0942856e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Random Forest - Train Accuracy: 1.0000\n",
      "ðŸŒ² Random Forest - Test Accuracy: 0.9975\n",
      "âš¡ XGBoost - Train Accuracy: 1.0000\n",
      "âš¡ XGBoost - Test Accuracy: 0.9825\n",
      "ðŸ§© SVM - Train Accuracy: 0.9988\n",
      "ðŸ§© SVM - Test Accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "# Checking for Overfitting\n",
    "# Checking train vs test accuracy for Random Forest\n",
    "rf_train_accuracy = rf.score(X_train_combined, y_train)\n",
    "rf_test_accuracy = accuracy_score(y_test, rf_preds)\n",
    "\n",
    "print(f\"ðŸŒ² Random Forest - Train Accuracy: {rf_train_accuracy:.4f}\")\n",
    "print(f\"ðŸŒ² Random Forest - Test Accuracy: {rf_test_accuracy:.4f}\")\n",
    "\n",
    "# Checking train vs test accuracy for XGBoost\n",
    "xgb_train_accuracy = xgb.score(X_train_combined, y_train)\n",
    "xgb_test_accuracy = accuracy_score(y_test, xgb_preds)\n",
    "\n",
    "print(f\"âš¡ XGBoost - Train Accuracy: {xgb_train_accuracy:.4f}\")\n",
    "print(f\"âš¡ XGBoost - Test Accuracy: {xgb_test_accuracy:.4f}\")\n",
    "\n",
    "# Checking train vs test accuracy for SVM\n",
    "svm_train_accuracy = svm.score(X_train_combined, y_train)\n",
    "svm_test_accuracy = accuracy_score(y_test, svm_preds)\n",
    "\n",
    "print(f\"ðŸ§© SVM - Train Accuracy: {svm_train_accuracy:.4f}\")\n",
    "print(f\"ðŸ§© SVM - Test Accuracy: {svm_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95e87685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Pretrained RoBERTa Model & Tokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)  # binary classification\n",
    "model.to('cuda')  # Ensure to use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3d506e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Dear Michael, I hope this message finds you well. We are reaching out to you with an urgent request regarding your account verification. As part of our commitment to ensuring the security of our users, we kindly ask you to verify your account at your earliest convenience. By completing this simple verification process, you not only enhance the security of your account but also contribute to making our community safer for everyone. Your cooperation in this matter is greatly appreciated. Please click [here](https://account-verification-link1.com) to proceed with the verification process. Thank you for your prompt attention to this matter. Best regards, Sarah Johnson Customer Support Team', 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1368.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the Dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load your cleaned CSV\n",
    "df = pd.read_csv(\"C:/Users/rugwe/OneDrive/Desktop/AI_LAB/Datasets/Final.csv\")  # Or provide full path if needed\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Optional: Show a few samples\n",
    "print(dataset[0])\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "tokenized_datasets.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e3fc3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20)\n",
    "train_dataset = tokenized_datasets.shuffle(seed=42).select([i for i in list(range(int(0.8 * len(tokenized_datasets))))])\n",
    "test_dataset = tokenized_datasets.select([i for i in list(range(int(0.8 * len(tokenized_datasets)), len(tokenized_datasets)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "89683a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "612297d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rugwe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Optimizer & Scheduler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * 3  # Train for 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dbd284f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "Epoch 1/3 - Loss: 0.0698 - Accuracy: 0.9688\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "Epoch 2/3 - Loss: 0.0039 - Accuracy: 0.9988\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n",
      "Epoch 3/3 - Loss: 0.0002 - Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Loop\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Training loop\n",
    "epochs = 3  # number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to train mode\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Move data to GPU if available\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        # Forward pass\n",
    "        print(batch.keys())  # Debugging: Check if 'input_ids' is in batch\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct_preds += (preds == batch['labels']).sum().item()\n",
    "        total_preds += batch['labels'].size(0)\n",
    "\n",
    "    # Calculate training loss and accuracy\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {avg_train_loss:.4f} - Accuracy: {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c04cc29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the Test Set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct_preds += (preds == batch['labels']).sum().item()\n",
    "        total_preds += batch['labels'].size(0)\n",
    "\n",
    "test_accuracy = correct_preds / total_preds\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e3504e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/roberta_finetuned_tokenizer\\\\tokenizer_config.json',\n",
       " 'models/roberta_finetuned_tokenizer\\\\special_tokens_map.json',\n",
       " 'models/roberta_finetuned_tokenizer\\\\vocab.json',\n",
       " 'models/roberta_finetuned_tokenizer\\\\merges.txt',\n",
       " 'models/roberta_finetuned_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"models/roberta_finetuned\")\n",
    "tokenizer.save_pretrained(\"models/roberta_finetuned_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME Setup\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "# Class names\n",
    "class_names = [\"Not Phishing\", \"Phishing\"]\n",
    "\n",
    "# LIME Explainer\n",
    "lime_explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Prediction wrapper for LIME\n",
    "def lime_predict(texts):\n",
    "    encodings = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "22e17183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain and Visualize with LIME\n",
    "def explain_with_lime(text):\n",
    "    exp = lime_explainer.explain_instance(text, lime_predict, num_features=10)\n",
    "\n",
    "    prediction = lime_predict([text])[0]\n",
    "    predicted_class = int(np.argmax(prediction))\n",
    "    confidence = float(prediction[predicted_class])\n",
    "\n",
    "    # Simple threshold for manual review\n",
    "    needs_manual_review = confidence < 0.7\n",
    "\n",
    "    return {\n",
    "        \"predicted_label\": class_names[predicted_class],\n",
    "        \"confidence\": round(confidence, 2),\n",
    "        \"needs_manual_review\": needs_manual_review,\n",
    "        \"lime_explanation\": exp\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7caa23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-Level Highlighting in Streamlit (Optional)\n",
    "import streamlit as st\n",
    "\n",
    "def display_lime_explanation(exp):\n",
    "    highlighted_text = exp.as_html()\n",
    "    st.markdown(\"#### ðŸ”Ž Token-level Explanation (LIME)\")\n",
    "    # st.components.v1.html(highlighted_text, height=400, scrolling=True)\n",
    "    print(exp.as_list())  # View raw token importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79c44b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
